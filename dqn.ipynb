{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDir(dir):\n",
    "    if not os.path.isdir(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            gamma,\n",
    "            epsilon,\n",
    "            numEpisodes,\n",
    "            stateDim,\n",
    "            actionDim,\n",
    "            replayBufferSize,\n",
    "            batchSize,\n",
    "            epsilonDecay,\n",
    "            epsilonDecayFrequency,\n",
    "            lr,\n",
    "            checkpointDir=\"checkpoints\",\n",
    "            checkpointFrequency = 5000,\n",
    "            runId = datetime.now().strftime(\"%y_%m_%d__%H_%M_%S\"),\n",
    "            inferenceFrequeny = 10,\n",
    "            terminationSteps = 500,\n",
    "            haltConditionChain = 2\n",
    "    ):\n",
    "        # Initialize DQN parameters and variables\n",
    "        self.env = env  # Environment\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Initial exploration rate\n",
    "        self.numEpisodes = numEpisodes  # Number of training episodes\n",
    "        self.stateDim = stateDim  # Dimensionality of state space\n",
    "        self.actionDim = actionDim  # Dimensionality of action space\n",
    "        self.replayBufferSize = replayBufferSize  # Size of the replay buffer\n",
    "        self.batchSize = batchSize  # Mini-batch size for training\n",
    "        self.epsilonDecay = epsilonDecay  # Rate of exploration decay\n",
    "        self.epsilonDecayFrequency = epsilonDecayFrequency  # Episode at which to start epsilon decay\n",
    "        self.checkpointDir = checkpointDir  # Folder for storing checkpoints\n",
    "        self.checkpointFrequency = checkpointFrequency  # Frequency (in steps) for storing checkpoints\n",
    "        self.runId = runId  # A run id to uniquely characterize every run / train attempt. By default takes current time as value\n",
    "        self.inferenceFrequency = inferenceFrequeny  # Period for running inference\n",
    "        self.writer = SummaryWriter()\n",
    "        self.terminationSteps = terminationSteps\n",
    "        self.terminated = False\n",
    "        self.haltConditionChain = haltConditionChain\n",
    "        self.haltChain = 0\n",
    "\n",
    "        # Initialize counters and lists\n",
    "        self.steps = 0\n",
    "        self.episodes = 0\n",
    "        self.sumRewardsEpisode = []  # Track sum of rewards per episode\n",
    "        self.inferenceRewards = []\n",
    "\n",
    "\n",
    "        # Initialize replay buffer\n",
    "        self.replayBuffer = deque(maxlen=self.replayBufferSize)\n",
    "\n",
    "        # Initialize neural network\n",
    "        self.network = self.createNetwork(self.stateDim, self.actionDim)\n",
    "\n",
    "        # Initialize actions history\n",
    "        self.actionsHistory = []\n",
    "\n",
    "        # Initialize optimizer\n",
    "        self.optim = torch.optim.Adam(self.network.parameters(), lr=lr)\n",
    "\n",
    "    # Define the neural network architecture\n",
    "    def createNetwork(self, inDim, outDim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(inDim, 64),  # Input layer with ReLU activation\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 64),  # Hidden layer with ReLU activation\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 32),  # Another hidden layer with ReLU activation\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, outDim),  # Output layer\n",
    "        ).to(device)\n",
    "    \n",
    "    # Define custom loss function for DQN\n",
    "    def my_loss(self, y_pred, y_true):\n",
    "        # Calculate mean squared error loss only for the actions taken\n",
    "        s1, s2 = y_true.shape\n",
    "        loss = F.mse_loss(y_pred[torch.arange(s1), self.actionsHistory], y_true[torch.arange(s1), self.actionsHistory])\n",
    "        return loss    \n",
    "    \n",
    "    # Train the network over multiple episodes\n",
    "    def trainingEpisodes(self):\n",
    "        # Initialize lists to store episode rewards and average Q values\n",
    "        self.sumRewardsEpisode = []\n",
    "        self.avgQ = []\n",
    "        # Iterate over episodes\n",
    "        for episode in tqdm(range(self.numEpisodes)):\n",
    "            rewards = []\n",
    "            # Reset environment and get initial state\n",
    "            (currentState, _) = self.env.reset()\n",
    "            terminalState = False\n",
    "            self.Qs = []  # List to store Q values\n",
    "            numSteps = 0\n",
    "            # Loop until episode termination\n",
    "            while not terminalState and numSteps < self.terminationSteps:\n",
    "                # Select action using epsilon-greedy policy\n",
    "                action = self.selectAction(currentState, episode)\n",
    "                # Take action and observe next state and reward\n",
    "                (nextState, reward, terminalState, _, _) = self.env.step(action)\n",
    "                rewards.append(reward)\n",
    "                # Store experience in replay buffer\n",
    "                self.replayBuffer.append((currentState, action, reward, nextState, terminalState))\n",
    "                # Train the network\n",
    "                self.trainNetwork()\n",
    "                currentState = nextState\n",
    "                numSteps += 1\n",
    "            \n",
    "            # Store episode reward and average Q value\n",
    "            self.sumRewardsEpisode.append(np.sum(rewards))\n",
    "            self.avgQ.append(np.mean(self.Qs))\n",
    "\n",
    "            # Updating tensor board\n",
    "            self.writer.add_scalar(\"steps\", self.steps, self.episodes)\n",
    "            self.writer.add_scalar(\"rewards\", self.sumRewardsEpisode[self.episodes], self.episodes)\n",
    "            self.writer.add_scalar(\"avg Q value\", self.avgQ[self.episodes], self.episodes)\n",
    "\n",
    "            self.episodes += 1\n",
    "\n",
    "            if self.episodes % self.inferenceFrequency == 0:\n",
    "                self.runInference()\n",
    "                self.writer.add_scalar(\"inference\", self.inferenceRewards[-1], self.episodes)\n",
    "\n",
    "            if self.terminated:\n",
    "                print(\"Terminated!\")\n",
    "                self.save_checkpoint()\n",
    "                return\n",
    "\n",
    "\n",
    "    \n",
    "    # Select action using epsilon-greedy policy\n",
    "    def selectAction(self, state, index):\n",
    "        self.network.eval()\n",
    "        # Initial exploration phase\n",
    "        if index == 0:\n",
    "            return np.random.choice(self.actionDim)\n",
    "        \n",
    "        rand = np.random.random()\n",
    "        # Decay exploration rate over time\n",
    "        if index % self.epsilonDecayFrequency == 0:\n",
    "            self.epsilon *= self.epsilonDecay\n",
    "\n",
    "        if rand < self.epsilon:\n",
    "            return np.random.choice(self.actionDim)  # Random action\n",
    "        # Exploit learned policy\n",
    "        Q = self.network(torch.tensor(state.reshape(1, 4)).to(device))\n",
    "        return torch.argmax(Q[0]).item()\n",
    "    \n",
    "    # Train the network using experiences from replay buffer\n",
    "    def trainNetwork(self):\n",
    "        if len(self.replayBuffer) > self.batchSize:\n",
    "            # Sample a mini-batch from replay buffer\n",
    "            randomBatch = random.sample(self.replayBuffer, self.batchSize)\n",
    "            currentStateBatch = torch.zeros(size=(self.batchSize, 4)).to(device)\n",
    "            nextStateBatch = torch.zeros(size=(self.batchSize, 4)).to(device)\n",
    "\n",
    "            for index, tupleS in enumerate(randomBatch):\n",
    "                # Extract data from the mini-batch\n",
    "                currentStateBatch[index, :] = torch.tensor(tupleS[0])\n",
    "                nextStateBatch[index, :] = torch.tensor(tupleS[3])\n",
    "\n",
    "            # Set network to evaluation mode\n",
    "            self.network.eval()\n",
    "            QS_ = self.network(nextStateBatch)  # Compute Q-values for next states\n",
    "\n",
    "            Y = torch.zeros((self.batchSize, 2)).to(device)\n",
    "            self.actionsHistory = []\n",
    "            rewards_batch = []\n",
    "            for idx, (currentState, action, reward, nextState, terminated) in enumerate(randomBatch):\n",
    "                rewards_batch.append(reward)\n",
    "                if terminated:\n",
    "                    y = reward\n",
    "                else:\n",
    "                    # Calculate target Q-value using target network\n",
    "                    y = reward + self.gamma * torch.max(QS_[idx])\n",
    "                self.actionsHistory.append(action)\n",
    "                Y[idx, action] = y\n",
    "            \n",
    "            # Set network back to training mode\n",
    "            self.network.train()\n",
    "\n",
    "            # Zero out gradients\n",
    "            self.optim.zero_grad()\n",
    "\n",
    "            # Compute Q-values for current states\n",
    "            QS = self.network(currentStateBatch).to(device)\n",
    "\n",
    "            # Compute loss and backpropagate\n",
    "            loss = self.my_loss(QS, Y)\n",
    "            loss.backward()\n",
    "            self.optim.step()  # Update network parameters\n",
    "\n",
    "            self.Qs.append(QS.flatten().sum().item())  # Track sum of Q-values\n",
    "\n",
    "            self.steps += 1  # Increment step counter\n",
    "\n",
    "            # Saving checkpoints periodically\n",
    "            if self.steps % self.checkpointFrequency == 0:\n",
    "                self.save_checkpoint()\n",
    "\n",
    "    def runInference(self):\n",
    "        self.network.eval()\n",
    "        # Initialize lists to store episode rewards and average Q values\n",
    "        # Iterate over episodes\n",
    "        \n",
    "        rewards = []\n",
    "        # Reset environment and get initial state\n",
    "        (currentState, _) = self.env.reset()\n",
    "        terminalState = False\n",
    "        self.Qs = []  # List to store Q values\n",
    "        numSteps = 0\n",
    "        # Loop until episode termination\n",
    "        while not terminalState and numSteps < self.terminationSteps:\n",
    "            # Select action using epsilon-greedy policy\n",
    "            Q = self.network(torch.tensor(currentState.reshape(1, 4)).to(device))\n",
    "            action =  torch.argmax(Q[0]).item()\n",
    "            # Take action and observe next state and reward\n",
    "            (nextState, reward, terminalState, _, _) = self.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            currentState = nextState\n",
    "            numSteps += 1\n",
    "            \n",
    "        # Store episode reward and average Q value\n",
    "        self.inferenceRewards.append(np.sum(rewards))\n",
    "\n",
    "        if numSteps == self.terminationSteps:\n",
    "            self.haltChain += 1\n",
    "        else:\n",
    "            self.haltChain = 0\n",
    "        \n",
    "        if self.haltChain == self.haltConditionChain:\n",
    "            self.terminated = True\n",
    "\n",
    "\n",
    "    # A function to save checkpoints\n",
    "    def save_checkpoint(self):\n",
    "        # !TODO self.writer and self.env is not picklable. this is a workaround I am not too happy with\n",
    "        tempWriter = self.writer\n",
    "        self.writer = None\n",
    "        tempEnv = self.env\n",
    "        self.env = None\n",
    "        \n",
    "\n",
    "        createDir(self.checkpointDir)\n",
    "        createDir(os.path.join(self.checkpointDir, self.runId))\n",
    "        checkpointPath = os.path.join(self.checkpointDir, self.runId, f\"checkpoint-{self.steps}.pkl\")\n",
    "        with open(checkpointPath, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "        self.writer = tempWriter\n",
    "        self.env = tempEnv\n",
    "    \n",
    "    # A class method to load a checkpoint\n",
    "    @classmethod\n",
    "    def load_checkpoint(cls, checkpointPath):\n",
    "        with open(checkpointPath, \"rb\") as f:\n",
    "            dqn = pickle.load(f)\n",
    "        return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(\n",
    "    env=env,\n",
    "    gamma=1,\n",
    "    epsilon=0.1,\n",
    "    numEpisodes=500,\n",
    "    stateDim=4,\n",
    "    actionDim=2,\n",
    "    replayBufferSize=512,\n",
    "    batchSize=128,\n",
    "    epsilonDecay=0.8,\n",
    "    epsilonDecayFrequency=100,\n",
    "    lr = 1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "- train, experiment\n",
    "- use image\n",
    "  - prepare dataset\n",
    "  - train backbone\n",
    "  - retrain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- Dropout significantly slows down training and is not worth it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = gym.make(\"CartPole-v1\", render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.env = env2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.runInference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26303"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn2 = DQN.load_checkpoint(\"./checkpoints/24_03_19__00_27_08/checkpoint-5000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn2.env = env2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn2.runInference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
