{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDir(dir):\n",
    "    if not os.path.isdir(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            gamma,\n",
    "            epsilon,\n",
    "            numEpisodes,\n",
    "            stateDim,\n",
    "            actionDim,\n",
    "            replayBufferSize,\n",
    "            batchSize,\n",
    "            epsilonDecay,\n",
    "            epsilonDecayStart,\n",
    "            lr,\n",
    "            checkpointDir=\"checkpoints\",\n",
    "            checkpointFrequency = 10000,\n",
    "            runId = datetime.now().strftime(\"%y_%m_%d__%H_%M_%S\"),\n",
    "    ):\n",
    "        # Initialize DQN parameters and variables\n",
    "        self.env = env  # Environment\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Initial exploration rate\n",
    "        self.numEpisodes = numEpisodes  # Number of training episodes\n",
    "        self.stateDim = stateDim  # Dimensionality of state space\n",
    "        self.actionDim = actionDim  # Dimensionality of action space\n",
    "        self.replayBufferSize = replayBufferSize  # Size of the replay buffer\n",
    "        self.batchSize = batchSize  # Mini-batch size for training\n",
    "        self.epsilonDecay = epsilonDecay  # Rate of exploration decay\n",
    "        self.epsilonDecayStart = epsilonDecayStart  # Episode at which to start epsilon decay\n",
    "        self.checkpointDir = checkpointDir  # Folder for storing checkpoints\n",
    "        self.checkpointFrequency = checkpointFrequency  # Frequency (in steps) for storing checkpoints\n",
    "        self.runId = runId  # A run id to uniquely characterize every run / train attempt. By default takes current time as value\n",
    " \n",
    "        self.writer = SummaryWriter()\n",
    "\n",
    "        # Initialize counters and lists\n",
    "        self.steps = 0\n",
    "        self.episodes = 0\n",
    "        self.sumRewardsEpisode = []  # Track sum of rewards per episode\n",
    "\n",
    "        # Initialize replay buffer\n",
    "        self.replayBuffer = deque(maxlen=self.replayBufferSize)\n",
    "\n",
    "        # Initialize neural network\n",
    "        self.network = self.createNetwork(self.stateDim, self.actionDim)\n",
    "\n",
    "        # Initialize actions history\n",
    "        self.actionsHistory = []\n",
    "\n",
    "        # Initialize optimizer\n",
    "        self.optim = torch.optim.Adam(self.network.parameters(), lr=lr)\n",
    "\n",
    "    # Define the neural network architecture\n",
    "    def createNetwork(self, inDim, outDim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(inDim, 64),  # Input layer with ReLU activation\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 64),  # Hidden layer with ReLU activation\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 32),  # Another hidden layer with ReLU activation\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, outDim),  # Output layer\n",
    "        ).to(device)\n",
    "    \n",
    "    # Define custom loss function for DQN\n",
    "    def my_loss(self, y_pred, y_true):\n",
    "        # Calculate mean squared error loss only for the actions taken\n",
    "        s1, s2 = y_true.shape\n",
    "        loss = F.mse_loss(y_pred[torch.arange(s1), self.actionsHistory], y_true[torch.arange(s1), self.actionsHistory])\n",
    "        return loss    \n",
    "    \n",
    "    # Train the network over multiple episodes\n",
    "    def trainingEpisodes(self):\n",
    "        # Initialize lists to store episode rewards and average Q values\n",
    "        self.sumRewardsEpisode = []\n",
    "        self.avgQ = []\n",
    "        # Iterate over episodes\n",
    "        for episode in tqdm(range(self.numEpisodes)):\n",
    "            rewards = []\n",
    "            # Reset environment and get initial state\n",
    "            (currentState, _) = self.env.reset()\n",
    "            terminalState = False\n",
    "            self.Qs = []  # List to store Q values\n",
    "            # Loop until episode termination\n",
    "            while not terminalState:\n",
    "                # Select action using epsilon-greedy policy\n",
    "                action = self.selectAction(currentState, episode)\n",
    "                # Take action and observe next state and reward\n",
    "                (nextState, reward, terminalState, _, _) = self.env.step(action)\n",
    "                rewards.append(reward)\n",
    "                # Store experience in replay buffer\n",
    "                self.replayBuffer.append((currentState, action, reward, nextState, terminalState))\n",
    "                # Train the network\n",
    "                self.trainNetwork()\n",
    "                currentState = nextState\n",
    "            \n",
    "            # Store episode reward and average Q value\n",
    "            self.sumRewardsEpisode.append(np.sum(rewards))\n",
    "            self.avgQ.append(np.mean(self.Qs))\n",
    "\n",
    "            # Updating tensor board\n",
    "            self.writer.add_scalar(\"steps\", self.steps, self.episodes)\n",
    "            self.writer.add_scalar(\"rewards\", self.sumRewardsEpisode[self.episodes], self.episodes)\n",
    "            self.writer.add_scalar(\"avg Q value\", self.avgQ[self.episodes], self.episodes)\n",
    "\n",
    "            self.episodes += 1\n",
    "    \n",
    "    # Select action using epsilon-greedy policy\n",
    "    def selectAction(self, state, index):\n",
    "        # Initial exploration phase\n",
    "        if index == 0:\n",
    "            return np.random.choice(self.actionDim)\n",
    "        \n",
    "        rand = np.random.random()\n",
    "        # Decay exploration rate over time\n",
    "        if index > self.epsilonDecayStart:\n",
    "            self.epsilon *= self.epsilonDecay\n",
    "\n",
    "        if rand < self.epsilon:\n",
    "            return np.random.choice(self.actionDim)  # Random action\n",
    "        # Exploit learned policy\n",
    "        Q = self.network(torch.tensor(state.reshape(1, 4)).to(device))\n",
    "        return torch.argmax(Q[0]).item()\n",
    "    \n",
    "    # Train the network using experiences from replay buffer\n",
    "    def trainNetwork(self):\n",
    "        if len(self.replayBuffer) > self.batchSize:\n",
    "            # Sample a mini-batch from replay buffer\n",
    "            randomBatch = random.sample(self.replayBuffer, self.batchSize)\n",
    "            currentStateBatch = torch.zeros(size=(self.batchSize, 4)).to(device)\n",
    "            nextStateBatch = torch.zeros(size=(self.batchSize, 4)).to(device)\n",
    "\n",
    "            for index, tupleS in enumerate(randomBatch):\n",
    "                # Extract data from the mini-batch\n",
    "                currentStateBatch[index, :] = torch.tensor(tupleS[0])\n",
    "                nextStateBatch[index, :] = torch.tensor(tupleS[3])\n",
    "\n",
    "            # Set network to evaluation mode\n",
    "            self.network.eval()\n",
    "            QS_ = self.network(nextStateBatch)  # Compute Q-values for next states\n",
    "\n",
    "            Y = torch.zeros((self.batchSize, 2)).to(device)\n",
    "            self.actionsHistory = []\n",
    "            rewards_batch = []\n",
    "            for idx, (currentState, action, reward, nextState, terminated) in enumerate(randomBatch):\n",
    "                rewards_batch.append(reward)\n",
    "                if terminated:\n",
    "                    y = reward\n",
    "                else:\n",
    "                    # Calculate target Q-value using target network\n",
    "                    y = reward + self.gamma * torch.max(QS_[idx])\n",
    "                self.actionsHistory.append(action)\n",
    "                Y[idx, action] = y\n",
    "            \n",
    "            # Set network back to training mode\n",
    "            self.network.train()\n",
    "\n",
    "            # Zero out gradients\n",
    "            self.optim.zero_grad()\n",
    "\n",
    "            # Compute Q-values for current states\n",
    "            QS = self.network(currentStateBatch).to(device)\n",
    "\n",
    "            # Compute loss and backpropagate\n",
    "            loss = self.my_loss(QS, Y)\n",
    "            loss.backward()\n",
    "            self.optim.step()  # Update network parameters\n",
    "\n",
    "            self.Qs.append(QS.flatten().sum().item())  # Track sum of Q-values\n",
    "\n",
    "            self.steps += 1  # Increment step counter\n",
    "\n",
    "            # Saving checkpoints periodically\n",
    "            if self.steps % self.checkpointFrequency == 0:\n",
    "                self.save_checkpoint()\n",
    "\n",
    "    # A function to save checkpoints\n",
    "    def save_checkpoint(self):\n",
    "        # !TODO self.writer is not picklable. this is a workaround I am not too happy with\n",
    "        tempWriter = self.writer\n",
    "        self.writer = None\n",
    "\n",
    "        createDir(self.checkpointDir)\n",
    "        createDir(os.path.join(self.checkpointDir, self.runId))\n",
    "        checkpointPath = os.path.join(self.checkpointDir, self.runId, f\"checkpoint-{self.steps}.pkl\")\n",
    "        with open(checkpointPath, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "        self.writer = tempWriter\n",
    "    \n",
    "    # A class method to load a checkpoint\n",
    "    @classmethod\n",
    "    def load_checkpoint(cls, checkpointPath):\n",
    "        with open(checkpointPath, \"rb\") as f:\n",
    "            dqn = pickle.load(f)\n",
    "        return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(\n",
    "    env=env,\n",
    "    gamma=1,\n",
    "    epsilon=0.1,\n",
    "    numEpisodes=1000,\n",
    "    stateDim=4,\n",
    "    actionDim=2,\n",
    "    replayBufferSize=256,\n",
    "    batchSize=64,\n",
    "    epsilonDecay=0.99,\n",
    "    epsilonDecayStart=200,\n",
    "    lr = 1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80605350affa4fbba4ded0799e4e2ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\projects\\pj_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "c:\\programming\\projects\\pj_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\programming\\projects\\pj_env\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainingEpisodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 97\u001b[0m, in \u001b[0;36mDQN.trainingEpisodes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplayBuffer\u001b[38;5;241m.\u001b[39mappend((currentState, action, reward, nextState, terminalState))\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# Train the network\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     currentState \u001b[38;5;241m=\u001b[39m nextState\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Store episode reward and average Q value\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 162\u001b[0m, in \u001b[0;36mDQN.trainNetwork\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Zero out gradients\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Compute Q-values for current states\u001b[39;00m\n\u001b[0;32m    165\u001b[0m QS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(currentStateBatch)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\programming\\projects\\pj_env\\lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\programming\\projects\\pj_env\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\programming\\projects\\pj_env\\lib\\site-packages\\torch\\optim\\optimizer.py:815\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    813\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 815\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_profile_name):\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\programming\\projects\\pj_env\\lib\\site-packages\\torch\\autograd\\profiler.py:605\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\programming\\projects\\pj_env\\lib\\site-packages\\torch\\_ops.py:755\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dqn.trainingEpisodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "- .py\n",
    "- params (yml)\n",
    "- train, experiment\n",
    "- use image\n",
    "  - prepare dataset\n",
    "  - train backbone\n",
    "  - retrain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- Dropout significantly slows down training and is not worth it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
