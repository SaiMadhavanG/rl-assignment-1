{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDir(dir):\n",
    "    if not os.path.isdir(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            gamma,\n",
    "            epsilon,\n",
    "            numEpisodes,\n",
    "            stateDim,\n",
    "            actionDim,\n",
    "            replayBufferSize,\n",
    "            batchSize,\n",
    "            epsilonDecay,\n",
    "            epsilonDecayFrequency,\n",
    "            lr,\n",
    "            checkpointDir=\"checkpoints\",\n",
    "            checkpointFrequency = 5000,\n",
    "            runId = datetime.now().strftime(\"%y_%m_%d__%H_%M_%S\"),\n",
    "            inferenceFrequeny = 10,\n",
    "            terminationSteps = 500,\n",
    "            haltConditionChain = 2\n",
    "    ):\n",
    "        # Initialize DQN parameters and variables\n",
    "        self.env = env  # Environment\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Initial exploration rate\n",
    "        self.numEpisodes = numEpisodes  # Number of training episodes\n",
    "        self.stateDim = stateDim  # Dimensionality of state space\n",
    "        self.actionDim = actionDim  # Dimensionality of action space\n",
    "        self.replayBufferSize = replayBufferSize  # Size of the replay buffer\n",
    "        self.batchSize = batchSize  # Mini-batch size for training\n",
    "        self.epsilonDecay = epsilonDecay  # Rate of exploration decay\n",
    "        self.epsilonDecayFrequency = epsilonDecayFrequency  # Episode at which to start epsilon decay\n",
    "        self.checkpointDir = checkpointDir  # Folder for storing checkpoints\n",
    "        self.checkpointFrequency = checkpointFrequency  # Frequency (in steps) for storing checkpoints\n",
    "        self.runId = runId  # A run id to uniquely characterize every run / train attempt. By default takes current time as value\n",
    "        self.inferenceFrequency = inferenceFrequeny  # Period for running inference\n",
    "        self.writer = SummaryWriter()\n",
    "        self.terminationSteps = terminationSteps\n",
    "        self.terminated = False\n",
    "        self.haltConditionChain = haltConditionChain\n",
    "        self.haltChain = 0\n",
    "\n",
    "        # Initialize counters and lists\n",
    "        self.steps = 0\n",
    "        self.episodes = 0\n",
    "        self.sumRewardsEpisode = []  # Track sum of rewards per episode\n",
    "        self.inferenceRewards = []\n",
    "\n",
    "\n",
    "        # Initialize replay buffer\n",
    "        self.replayBuffer = deque(maxlen=self.replayBufferSize)\n",
    "\n",
    "        # Initialize neural network\n",
    "        self.network = self.createNetwork(self.stateDim, self.actionDim)\n",
    "\n",
    "        # Initialize actions history\n",
    "        self.actionsHistory = []\n",
    "\n",
    "        # Initialize optimizer\n",
    "        self.optim = torch.optim.Adam(self.network.parameters(), lr=lr)\n",
    "\n",
    "    # Define the neural network architecture\n",
    "    def createNetwork(self, inDim, outDim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(inDim, 64),  # Input layer with ReLU activation\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 64),  # Hidden layer with ReLU activation\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 32),  # Another hidden layer with ReLU activation\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, outDim),  # Output layer\n",
    "        ).to(device)\n",
    "    \n",
    "    # Define custom loss function for DQN\n",
    "    def my_loss(self, y_pred, y_true):\n",
    "        # Calculate mean squared error loss only for the actions taken\n",
    "        s1, s2 = y_true.shape\n",
    "        loss = F.mse_loss(y_pred[torch.arange(s1), self.actionsHistory], y_true[torch.arange(s1), self.actionsHistory])\n",
    "        return loss    \n",
    "    \n",
    "    # Train the network over multiple episodes\n",
    "    def trainingEpisodes(self):\n",
    "        # Initialize lists to store episode rewards and average Q values\n",
    "        self.sumRewardsEpisode = []\n",
    "        self.avgQ = []\n",
    "        # Iterate over episodes\n",
    "        for episode in tqdm(range(self.numEpisodes)):\n",
    "            rewards = []\n",
    "            # Reset environment and get initial state\n",
    "            (currentState, _) = self.env.reset()\n",
    "            terminalState = False\n",
    "            self.Qs = []  # List to store Q values\n",
    "            numSteps = 0\n",
    "            # Loop until episode termination\n",
    "            while not terminalState and numSteps < self.terminationSteps:\n",
    "                # Select action using epsilon-greedy policy\n",
    "                action = self.selectAction(currentState, episode)\n",
    "                # Take action and observe next state and reward\n",
    "                (nextState, reward, terminalState, _, _) = self.env.step(action)\n",
    "                rewards.append(reward)\n",
    "                # Store experience in replay buffer\n",
    "                self.replayBuffer.append((currentState, action, reward, nextState, terminalState))\n",
    "                # Train the network\n",
    "                self.trainNetwork()\n",
    "                currentState = nextState\n",
    "                numSteps += 1\n",
    "            \n",
    "            # Store episode reward and average Q value\n",
    "            self.sumRewardsEpisode.append(np.sum(rewards))\n",
    "            self.avgQ.append(np.mean(self.Qs))\n",
    "\n",
    "            # Updating tensor board\n",
    "            self.writer.add_scalar(\"steps\", self.steps, self.episodes)\n",
    "            self.writer.add_scalar(\"rewards\", self.sumRewardsEpisode[self.episodes], self.episodes)\n",
    "            self.writer.add_scalar(\"avg Q value\", self.avgQ[self.episodes], self.episodes)\n",
    "\n",
    "            self.episodes += 1\n",
    "\n",
    "            if self.episodes % self.inferenceFrequency == 0:\n",
    "                self.runInference()\n",
    "                self.writer.add_scalar(\"inference\", self.inferenceRewards[-1], self.episodes)\n",
    "\n",
    "            if self.terminated:\n",
    "                print(\"Terminated!\")\n",
    "                self.save_checkpoint()\n",
    "                return\n",
    "\n",
    "\n",
    "    \n",
    "    # Select action using epsilon-greedy policy\n",
    "    def selectAction(self, state, index):\n",
    "        self.network.eval()\n",
    "        # Initial exploration phase\n",
    "        if index == 0:\n",
    "            return np.random.choice(self.actionDim)\n",
    "        \n",
    "        rand = np.random.random()\n",
    "        # Decay exploration rate over time\n",
    "        if index % self.epsilonDecayFrequency == 0:\n",
    "            self.epsilon *= self.epsilonDecay\n",
    "\n",
    "        if rand < self.epsilon:\n",
    "            return np.random.choice(self.actionDim)  # Random action\n",
    "        # Exploit learned policy\n",
    "        Q = self.network(torch.tensor(state.reshape(1, 4)).to(device))\n",
    "        return torch.argmax(Q[0]).item()\n",
    "    \n",
    "    # Train the network using experiences from replay buffer\n",
    "    def trainNetwork(self):\n",
    "        if len(self.replayBuffer) > self.batchSize:\n",
    "            # Sample a mini-batch from replay buffer\n",
    "            randomBatch = random.sample(self.replayBuffer, self.batchSize)\n",
    "            currentStateBatch = torch.zeros(size=(self.batchSize, 4)).to(device)\n",
    "            nextStateBatch = torch.zeros(size=(self.batchSize, 4)).to(device)\n",
    "\n",
    "            for index, tupleS in enumerate(randomBatch):\n",
    "                # Extract data from the mini-batch\n",
    "                currentStateBatch[index, :] = torch.tensor(tupleS[0])\n",
    "                nextStateBatch[index, :] = torch.tensor(tupleS[3])\n",
    "\n",
    "            # Set network to evaluation mode\n",
    "            self.network.eval()\n",
    "            QS_ = self.network(nextStateBatch)  # Compute Q-values for next states\n",
    "\n",
    "            Y = torch.zeros((self.batchSize, 2)).to(device)\n",
    "            self.actionsHistory = []\n",
    "            rewards_batch = []\n",
    "            for idx, (currentState, action, reward, nextState, terminated) in enumerate(randomBatch):\n",
    "                rewards_batch.append(reward)\n",
    "                if terminated:\n",
    "                    y = reward\n",
    "                else:\n",
    "                    # Calculate target Q-value using target network\n",
    "                    y = reward + self.gamma * torch.max(QS_[idx])\n",
    "                self.actionsHistory.append(action)\n",
    "                Y[idx, action] = y\n",
    "            \n",
    "            # Set network back to training mode\n",
    "            self.network.train()\n",
    "\n",
    "            # Zero out gradients\n",
    "            self.optim.zero_grad()\n",
    "\n",
    "            # Compute Q-values for current states\n",
    "            QS = self.network(currentStateBatch).to(device)\n",
    "\n",
    "            # Compute loss and backpropagate\n",
    "            loss = self.my_loss(QS, Y)\n",
    "            loss.backward()\n",
    "            self.optim.step()  # Update network parameters\n",
    "\n",
    "            self.Qs.append(QS.flatten().sum().item())  # Track sum of Q-values\n",
    "\n",
    "            self.steps += 1  # Increment step counter\n",
    "\n",
    "            # Saving checkpoints periodically\n",
    "            if self.steps % self.checkpointFrequency == 0:\n",
    "                self.save_checkpoint()\n",
    "\n",
    "    def runInference(self):\n",
    "        self.network.eval()\n",
    "        # Initialize lists to store episode rewards and average Q values\n",
    "        # Iterate over episodes\n",
    "        \n",
    "        rewards = []\n",
    "        # Reset environment and get initial state\n",
    "        (currentState, _) = self.env.reset()\n",
    "        terminalState = False\n",
    "        self.Qs = []  # List to store Q values\n",
    "        numSteps = 0\n",
    "        # Loop until episode termination\n",
    "        while not terminalState and numSteps < self.terminationSteps:\n",
    "            # Select action using epsilon-greedy policy\n",
    "            Q = self.network(torch.tensor(currentState.reshape(1, 4)).to(device))\n",
    "            action =  torch.argmax(Q[0]).item()\n",
    "            # Take action and observe next state and reward\n",
    "            (nextState, reward, terminalState, _, _) = self.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            currentState = nextState\n",
    "            numSteps += 1\n",
    "            \n",
    "        # Store episode reward and average Q value\n",
    "        self.inferenceRewards.append(np.sum(rewards))\n",
    "\n",
    "        if numSteps == self.terminationSteps:\n",
    "            self.haltChain += 1\n",
    "        else:\n",
    "            self.haltChain = 0\n",
    "        \n",
    "        if self.haltChain == self.haltConditionChain:\n",
    "            self.terminated = True\n",
    "\n",
    "\n",
    "    # A function to save checkpoints\n",
    "    def save_checkpoint(self):\n",
    "        # !TODO self.writer and self.env is not picklable. this is a workaround I am not too happy with\n",
    "        tempWriter = self.writer\n",
    "        self.writer = None\n",
    "        tempEnv = self.env\n",
    "        self.env = None\n",
    "        \n",
    "\n",
    "        createDir(self.checkpointDir)\n",
    "        createDir(os.path.join(self.checkpointDir, self.runId))\n",
    "        checkpointPath = os.path.join(self.checkpointDir, self.runId, f\"checkpoint-{self.steps}.pkl\")\n",
    "        with open(checkpointPath, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "        self.writer = tempWriter\n",
    "        self.env = tempEnv\n",
    "    \n",
    "    # A class method to load a checkpoint\n",
    "    @classmethod\n",
    "    def load_checkpoint(cls, checkpointPath):\n",
    "        with open(checkpointPath, \"rb\") as f:\n",
    "            dqn = pickle.load(f)\n",
    "        return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(\n",
    "    env=env,\n",
    "    gamma=1,\n",
    "    epsilon=0.1,\n",
    "    numEpisodes=500,\n",
    "    stateDim=4,\n",
    "    actionDim=2,\n",
    "    replayBufferSize=512,\n",
    "    batchSize=128,\n",
    "    epsilonDecay=0.8,\n",
    "    epsilonDecayFrequency=100,\n",
    "    lr = 1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7954b08dee3849d9af4260f2d085b250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\projects\\pj_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "c:\\programming\\projects\\pj_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\programming\\projects\\pj_env\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainingEpisodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 107\u001b[0m, in \u001b[0;36mDQN.trainingEpisodes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplayBuffer\u001b[38;5;241m.\u001b[39mappend((currentState, action, reward, nextState, terminalState))\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Train the network\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m currentState \u001b[38;5;241m=\u001b[39m nextState\n\u001b[0;32m    109\u001b[0m numSteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 191\u001b[0m, in \u001b[0;36mDQN.trainNetwork\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Compute loss and backpropagate\u001b[39;00m\n\u001b[0;32m    190\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmy_loss(QS, Y)\n\u001b[1;32m--> 191\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update network parameters\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQs\u001b[38;5;241m.\u001b[39mappend(QS\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem())  \u001b[38;5;66;03m# Track sum of Q-values\u001b[39;00m\n",
      "File \u001b[1;32mc:\\programming\\projects\\pj_env\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\programming\\projects\\pj_env\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dqn.trainingEpisodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "- termination condition\n",
    "- .py\n",
    "- params (yml)\n",
    "- train, experiment\n",
    "- use image\n",
    "  - prepare dataset\n",
    "  - train backbone\n",
    "  - retrain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- Dropout significantly slows down training and is not worth it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = gym.make(\"CartPole-v1\", render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.env = env2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.runInference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26303"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn2 = DQN.load_checkpoint(\"./checkpoints/24_03_19__00_27_08/checkpoint-5000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn2.env = env2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn2.runInference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
